{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook was used to clean the combined chat gpt output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file has ALL the comments that have been run through chat gpt\n",
    "all_comments = pd.read_csv(\"../data/reddit/gpt_output_coded/ALL_CODED_COMMENTS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Only Relevant Safety Comments\n",
    "As determined by OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5369, 10)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all data with gpt code 0: not safety related\n",
    "safety_comments = all_comments.drop(all_comments[all_comments[\"gpt_output\"] == '0'].index)\n",
    "# How many comments do we have left?\n",
    "safety_comments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split safety code with sentiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1  \\n8', '1  \\n4', '1  \\n7', '1  \\n6', '1  \\n3', '1  \\n5', '1, 5',\n",
       "       '1, 7', '1, 8', '1, 3', '1  \\n2', '1, 2', '1, 6', '1, 4', '1, 1',\n",
       "       '1, 9', '1  \\n9', '1  \\n10', '1  \\n1', '1,2', '1,3', '1,4', '1,6',\n",
       "       '1, 10', '1,5', '1,7', '1,8'], dtype=object)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See all the different encodings - some use newlines, some commas\n",
    "safety_comments[\"gpt_output\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1 8', '1 4', '1 7', '1 6', '1 3', '1 5', '1  5', '1  7', '1  8',\n",
       "       '1  3', '1 2', '1  2', '1  6', '1  4', '1  1', '1  9', '1 9',\n",
       "       '1 10', '1 1', '1  10'], dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all the newlines with commas so everything is in the same format\n",
    "safety_comments[\"gpt_output\"] = [str(item).replace('  \\n', ' ') for item in safety_comments[\"gpt_output\"]]\n",
    "# replace all the commas with spaces\n",
    "safety_comments[\"gpt_output\"] = [str(item).replace(',', ' ') for item in safety_comments[\"gpt_output\"]]\n",
    "\n",
    "safety_comments[\"gpt_output\"].unique() # notice some have a space and some dont "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '8'],\n",
       " ['1', '5'],\n",
       " ['1', '7'],\n",
       " ['1', '8'],\n",
       " ['1', '5'],\n",
       " ['1', '8'],\n",
       " ['1', '', '5'],\n",
       " ['1', '', '7'],\n",
       " ['1', '4'],\n",
       " ['1', '4']]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the column on commas\n",
    "split_scores = [entry.split(' ') for entry in safety_comments[\"gpt_output\"]]\n",
    "split_scores[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab and Append Safety Sentiment Scores to Safety Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab only the last item of the split lists\n",
    "sentiments = [int(lst[-1]) for lst in split_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column in dataframe\n",
    "safety_comments[\"sentiment_score\"] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop gpt output column\n",
    "safety_comments = safety_comments.drop(\"gpt_output\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>type</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ita0jq1</td>\n",
       "      <td>y57cbw</td>\n",
       "      <td>samthehaggis</td>\n",
       "      <td>Agreed to Cleveland Park and Woodley Park. The...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i9cnmct</td>\n",
       "      <td>utr8aw</td>\n",
       "      <td>dans_cafe</td>\n",
       "      <td>Generally speaking, robberies and muggings are...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i9csov5</td>\n",
       "      <td>utr8aw</td>\n",
       "      <td>Ok_Priority_1534</td>\n",
       "      <td>Downtown/Central DC.\\n\\nAs I'm looking at diff...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i9czvva</td>\n",
       "      <td>utr8aw</td>\n",
       "      <td>dans_cafe</td>\n",
       "      <td>of course there are.  And probably, it's over-...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cqarv2e</td>\n",
       "      <td>32cqd6</td>\n",
       "      <td>AUBlazin</td>\n",
       "      <td>Haha well you have decided to live in the sket...</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id submission_id            author  \\\n",
       "5     ita0jq1        y57cbw      samthehaggis   \n",
       "10    i9cnmct        utr8aw         dans_cafe   \n",
       "11    i9csov5        utr8aw  Ok_Priority_1534   \n",
       "12    i9czvva        utr8aw         dans_cafe   \n",
       "13    cqarv2e        32cqd6          AUBlazin   \n",
       "\n",
       "                                                 body  score  year  month  \\\n",
       "5   Agreed to Cleveland Park and Woodley Park. The...      1  2022     10   \n",
       "10  Generally speaking, robberies and muggings are...      1  2022      5   \n",
       "11  Downtown/Central DC.\\n\\nAs I'm looking at diff...      1  2022      5   \n",
       "12  of course there are.  And probably, it's over-...      1  2022      5   \n",
       "13  Haha well you have decided to live in the sket...      1  2015      4   \n",
       "\n",
       "    subreddit  type  sentiment_score  \n",
       "5           0     1                8  \n",
       "10          0     1                4  \n",
       "11          0     1                7  \n",
       "12          0     1                7  \n",
       "13          0     1                6  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "safety_comments.to_csv(\"../data/reddit/cleaned_sentiment_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
